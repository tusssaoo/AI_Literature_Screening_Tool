# AI文献筛选平台 - 本地模型版

一个本地运行的AI文献筛选平台，使用Ollama运行开源大模型，无需API Key，不用给云端服务交米。

## 功能特点

1. **本地AI模型**：使用Ollama本地运行开源大模型，数据不上传云端
2. **模型管理**：内置7个精选模型，一键下载，实时显示下载速率和进度
3. **PubMed导出文献解析**：目前只支持PubMed格式，直接上传PubMed格式的txt文件，自动提取标题摘要关键词等信息
5. **批量AI筛选**：逐篇分析文献，输出"纳入"或"排除"的判断结果及理由
6. **结果导出**：将筛选结果的新列作为新列导出到Excel

## 支持的模型

理论上Ollama支持的都能支持，目前先支持自动下载以下模型（qwen3:4b表现就很好了）

| 模型 | 大小 | 全名 |
|------|------|------|
| **qwen3:4b** | 2.5GB | 通义千问3 (4B) |
| **qwen3:8b** | 5GB | 通义千问3 (8B) |
| **qwen3:30b** | 18GB | 通义千问3 (30B) |
| **gemma3:1b** | 800MB | Gemma 3 (1B) |
| **gemma3:4b** | 2.5GB | Gemma 3 (4B) |
| **gemma3:12b** | 8GB | Gemma 3 (12B) |
| **gemma3:27b** | 16GB | Gemma 3 (27B) |

## 快速开始：
1.安装ollama

  去ollama官网下载即可。记得修改ollama模型库路径，移出默认的C盘

2.使用启动器

  双击文件夹的start_launcher.bat打开启动器

启动器功能：
- **📦 检查环境**：自动检查所需依赖是否安装（需要手动装的只有ollama）
- **🚀 启动应用**：启动服务并自动打开浏览器
- **⏹  停止应用**：释放应用服务占用的资源
- **若ollama已启动但是启动器显示未安装，关闭启动器重新打开即可**

## 使用流程

### 第一步：下载本地模型

1. 确保Ollama服务已启动（`ollama serve`）
2. 在平台左侧"模型库"区域查看可用模型
3. 点击想要使用的模型进行下载
4. 等待下载完成，下载过程中会显示实时速率和进度

### 第二步：上传文献

1. 导出PubMed格式的txt文件
2. 在平台中间区域点击"选择文件"，上传文件；或者将文件拖入
3. 若文件正常，系统会自动提取信息
4. 可以预览文献的解析结果

### 第三步：设置筛选标准

1. 在"选择模型"下拉框中选择已下载的模型
2. 在"输入筛选标准"文本框中输入你的筛选标准，例如：
   ```
   - 研究必须是随机对照试验
   - 研究对象必须是成年人
   - 排除病例报告
   ```
3.（非必须，但建议）系统会自动生成给强力云端AI修改的完整提示词（可在"完整提示词"区域查看和编辑），复制后粘贴到豆包等强大的云端AI，再复制优化后的提示词到"AI优化后的筛选标准"文本框内。

4. 能熟练拷打AI的也可以自己输入筛选标准到"AI优化后的筛选标准"文本框内。

### 第四步：批量筛选

1. 确认提示词无误后，点击"开始筛选"
2. 系统会逐篇将文献信息发送给本地AI进行分析
3. AI觉得正向证据充分会返回"符合"，负向证据充分会返回"不符合"，正向和负向证据均不充分返回"未知"的判断结果。并且返回判断理由
4. 筛选完成后，会显示统计信息和详细结果
5. 点击"下载筛选结果Excel"导出最终结果

## 界面说明

- **左侧栏**：模型库，显示可下载和已安装的模型
- **中间栏**：文献上传、筛选标准设置和结果展示
- **右侧栏**：分析进度和日志输出

## 系统要求

-最好有独显，效率比CPU高一些并且不容易卡

- **操作系统**: 目前公开版本只适配Windows（我的MAC能跑，但还没整明白怎么给其他人的MAC分发）
- **内存**: 建议8GB以上（运行4B模型）
- **硬盘**: 至少20GB可用空间（用于存储模型）
- **网络**: 首次下载模型需要联网

## 文件结构

```
ai文献筛选平台/
├── app.py                 # 主应用文件
├── requirements.txt       # 依赖列表
├── README.md              # 使用说明
├── start_launcher.bat     # Windows启动器
├── launcher/              # 启动器模块
│   ├── main.py
│   ├── config.py
│   └── ...
├── templates/
│   └── index.html         # 前端页面
├── uploads/               # 上传文件目录
└── outputs/               # 输出文件目录
```

## 技术栈

- **后端**: Python + Flask
- **前端**: HTML + CSS + JavaScript
- **数据处理**: Pandas, OpenPyXL
- **AI引擎**: Ollama + 开源大模型

## 隐私说明

- 所有数据处理都在本地完成
- 文献内容不会上传到任何云端API
- 模型运行在本地，无需网络连接（下载后）

## 软件许可

-仅供个人学习交流使用，反对一切学术不端行为


